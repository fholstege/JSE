{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "from JSE.data import *\n",
    "from JSE.settings import data_info, optimizer_info\n",
    "from JSE.models import *\n",
    "from JSE.training import *\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_JSE_for_concept(data_obj,device, dataset, seed, batch_size, optimizer_settings, lr, weight_decay, epochs, alpha, expected_diff, early_stopping, per_step=50, null_is_concept=False, eval_balanced=True, solver='SGD', balanced_weights_concept=True, which_concept='First'):\n",
    "    # Load the data\n",
    "    X_train, X_val, X_test = data_obj.X_train, data_obj.X_val, data_obj.X_test\n",
    "    y_m_test = data_obj.y_m_test\n",
    "    y_c_test = data_obj.y_c_test\n",
    "    y_c_2_train = data_obj.y_c_2_train\n",
    "    y_c_2_val = data_obj.y_c_2_val\n",
    "    y_c_2_test = data_obj.y_c_2_test\n",
    "\n",
    "\n",
    "    # set y_c_train, y_c_val, y_c_test to y_c_2_train, y_c_2_val, y_c_2_test\n",
    "    if which_concept == 'Second':\n",
    "        data_obj.y_c_train = y_c_2_train.clone()\n",
    "        data_obj.y_c_val = y_c_2_val.clone()\n",
    "        data_obj.y_c_test = y_c_2_test.clone()\n",
    "\n",
    "    # define the weights for the second concept\n",
    "    if balanced_weights_concept:\n",
    "        if which_concept == 'First':\n",
    "            concept_weights_train, concept_weights_val = data_obj.get_class_weights_train_val(y_c_test, y_c_test)\n",
    "        else:\n",
    "            concept_weights_train, concept_weights_val = data_obj.get_class_weights_train_val(y_c_2_train, y_c_2_val)\n",
    "        \n",
    "        include_weights = True\n",
    "    else:\n",
    "        concept_weights_train, concept_weights_val = None, None\n",
    "        include_weights = False\n",
    "\n",
    "    # define the loaders\n",
    "    concept_first = True\n",
    "    loaders = data_obj.create_loaders(batch_size=batch_size, workers=0, with_concept=True, include_weights=include_weights, train_weights=concept_weights_train, val_weights=concept_weights_val,concept_first=concept_first )\n",
    "\n",
    "    \n",
    "    # Run the JSE algorithm\n",
    "    set_seed(seed)\n",
    "    V_c_1, V_m_1, d_c_1, d_m_1 = train_JSE(data_obj,\n",
    "                                                            device=device,\n",
    "                                                            batch_size=batch_size, \n",
    "                                                            solver=solver,\n",
    "                                                            lr=lr,\n",
    "                                                            per_step=per_step,\n",
    "                                                            tol=optimizer_settings['tol'],\n",
    "                                                            early_stopping=early_stopping,\n",
    "                                                            patience=optimizer_settings['patience'],\n",
    "                                                            epochs=epochs, \n",
    "                                                            Delta = expected_diff,\n",
    "                                                            alpha=alpha,\n",
    "                                                            null_is_concept = null_is_concept,\n",
    "                                                            eval_balanced=eval_balanced, \n",
    "                                                            weight_decay=weight_decay,\n",
    "                                                            include_weights=include_weights,\n",
    "                                                            train_weights=concept_weights_train,\n",
    "                                                            val_weights=concept_weights_val,\n",
    "                                                            model_base_name='JSE_first_part_'+str(seed)+'_',\n",
    "                                                            concept_first=concept_first,\n",
    "                                                            )\n",
    "    \n",
    "    # use the V_c from the outer loop\n",
    "    d = V_c_1.shape[0]\n",
    "    P_c_1_orth = torch.eye(d) - create_P(V_c_1) \n",
    "\n",
    "    # transform the data for the first concept\n",
    "    X_train_transformed_1 = torch.matmul(X_train, P_c_1_orth)\n",
    "    X_val_transformed_1 = torch.matmul(X_val, P_c_1_orth)\n",
    "    X_test_transformed_1 = torch.matmul(X_test, P_c_1_orth)\n",
    "    data_obj.reset_X(X_train_transformed_1, X_val_transformed_1, batch_size=batch_size, reset_X_objects=True, include_weights=False, train_weights = None, val_weights = None, only_main=True)\n",
    "\n",
    "    # Train the model to get main\n",
    "    settings = 'standard_ERM_settings'\n",
    "    lr_ERM = optimizer_info[settings][dataset]['lr']\n",
    "    weight_decay_ERM = optimizer_info[settings][dataset]['weight_decay']\n",
    "\n",
    "     # Train the model to get main\n",
    "    set_seed(seed)\n",
    "    main_model = return_linear_model(d, \n",
    "                                                data_obj.main_loader,\n",
    "                                                device,\n",
    "                                                solver = solver,\n",
    "                                                lr=lr_ERM,\n",
    "                                                per_step=per_step,\n",
    "                                                tol = optimizer_settings['tol'],\n",
    "                                                early_stopping = early_stopping,\n",
    "                                                patience = optimizer_settings['patience'],\n",
    "                                                epochs = epochs,\n",
    "                                                bias=True,\n",
    "                                                weight_decay=weight_decay_ERM, \n",
    "                                                model_name='main_model_after_JSE_second_concept'+str(seed),\n",
    "                                                save_best_model=True\n",
    "                                                )\n",
    "        \n",
    "\n",
    "    # get the accuracy of the main model\n",
    "    y_m_pred_test = main_model(X_test_transformed_1)\n",
    "\n",
    "    # get the accuracy of the main model overall \n",
    "    main_acc_after = get_acc_pytorch_model(y_m_test, y_m_pred_test)\n",
    "\n",
    "    # get the accuracy of the main model per group - first concept\n",
    "    result_per_group_1, _ = get_acc_per_group(y_m_pred_test, y_m_test, y_c_test)\n",
    "\n",
    "    # get the accuracy of the main model per group - second concept\n",
    "    result_per_group_2, _ = get_acc_per_group(y_m_pred_test, y_m_test, y_c_2_test)\n",
    "\n",
    "    # save the results dict\n",
    "    results_dict = {\n",
    "        'main_acc_after': main_acc_after,\n",
    "        'result_per_group_1': result_per_group_1,\n",
    "        'result_per_group_2': result_per_group_2,\n",
    "        'V_c': V_c_1,\n",
    "    }\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'celebA'\n",
    "dataset_setting = 'sampled_data_two_concepts'\n",
    "device = 'cpu'\n",
    "spurious_ratio=0.8\n",
    "single_finetuned_model = False\n",
    "demean=True\n",
    "pca=True \n",
    "k_components=300\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "weight_decay = 0.001\n",
    "epochs = 50\n",
    "alpha = 0.05\n",
    "expected_diff = 0.0\n",
    "early_stopping = True\n",
    "\n",
    " # set the settings for dataset\n",
    "dataset_settings = data_info[dataset][dataset_setting]\n",
    "optimizer_settings = optimizer_info['All']\n",
    "\n",
    "\n",
    "list_results_JSE_first_concept = []\n",
    "for seed in range(0, 5):\n",
    "    # get the data obj for the first concept\n",
    "    set_seed(seed)\n",
    "    data_obj_first_concept = get_dataset_obj(dataset, dataset_settings, spurious_ratio, data_info, seed, device,  single_model_for_embeddings=single_finetuned_model)\n",
    "\n",
    "    # demean, pca for the first concept\n",
    "    if demean:\n",
    "        data_obj_first_concept.demean_X(reset_mean=True, include_test=True)\n",
    "    if pca:\n",
    "        data_obj_first_concept.transform_data_to_k_components(k_components, reset_V_k=True, include_test=True)\n",
    "        V_k_train = data_obj_first_concept.V_k_train\n",
    "\n",
    "    # run JSE for the first concept\n",
    "    result_dict_JSE_first_concept = run_JSE_for_concept(data_obj_first_concept,device, 'celebA', seed, batch_size, optimizer_settings, lr, weight_decay, epochs, alpha, expected_diff, early_stopping, which_concept='First', balanced_weights_concept=False)\n",
    "    list_results_JSE_first_concept.append(result_dict_JSE_first_concept)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'celebA'\n",
    "dataset_setting = 'sampled_data_two_concepts'\n",
    "device = 'cpu'\n",
    "spurious_ratio=0.8\n",
    "single_finetuned_model = False\n",
    "demean=True\n",
    "pca=True \n",
    "k_components=300\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "weight_decay = 0.01\n",
    "epochs = 50\n",
    "alpha = 0.05\n",
    "expected_diff = 0.0\n",
    "early_stopping = True\n",
    "\n",
    " # set the settings for dataset\n",
    "dataset_settings = data_info[dataset][dataset_setting]\n",
    "optimizer_settings = optimizer_info['All']\n",
    "\n",
    "\n",
    "list_results_JSE_second_concept = []\n",
    "for seed in range(0, 5):\n",
    "    # get the data obj for the second concept\n",
    "    set_seed(seed)\n",
    "    data_obj_second_concept = get_dataset_obj(dataset, dataset_settings, spurious_ratio, data_info, seed, device,  single_model_for_embeddings=single_finetuned_model)\n",
    "\n",
    "    # demean, pca for the second concept\n",
    "    if demean:\n",
    "        data_obj_second_concept.demean_X(reset_mean=True, include_test=True)\n",
    "    if pca:\n",
    "        data_obj_second_concept.transform_data_to_k_components(k_components, reset_V_k=True, include_test=True)\n",
    "        V_k_train = data_obj_second_concept.V_k_train\n",
    "\n",
    "    # run JSE for the second concept\n",
    "    result_dict_JSE_second_concept = run_JSE_for_concept(data_obj_second_concept,device, 'celebA', seed, batch_size, optimizer_settings, lr, weight_decay, epochs, alpha, expected_diff, early_stopping, which_concept='Second', balanced_weights_concept=True)\n",
    "    list_results_JSE_second_concept.append(result_dict_JSE_second_concept)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'celebA'\n",
    "dataset_setting = 'sampled_data_two_concepts'\n",
    "device = 'cpu'\n",
    "spurious_ratio=0.8\n",
    "single_finetuned_model = False\n",
    "demean=True\n",
    "pca=True \n",
    "k_components=300\n",
    "batch_size = 128\n",
    "settings = 'standard_ERM_settings'\n",
    "lr_ERM = optimizer_info[settings][dataset]['lr']\n",
    "weight_decay_ERM = optimizer_info[settings][dataset]['weight_decay']\n",
    "epochs = 50\n",
    "alpha = 0.05\n",
    "expected_diff = 0.0\n",
    "early_stopping = True\n",
    "solver='SGD'\n",
    "per_step=50\n",
    "\n",
    "\n",
    " # set the settings for dataset\n",
    "dataset_settings = data_info[dataset][dataset_setting]\n",
    "optimizer_settings = optimizer_info['All']\n",
    "\n",
    "\n",
    "# get the data obj\n",
    "list_results_JSE_both = []\n",
    "for seed in range(0, 5):\n",
    "\n",
    "    # get the data obj\n",
    "    set_seed(seed)\n",
    "    data_obj = get_dataset_obj(dataset, dataset_settings, spurious_ratio, data_info, seed, device,  single_model_for_embeddings=single_finetuned_model)\n",
    "\n",
    "    # demean, pca\n",
    "    if demean:\n",
    "        data_obj.demean_X(reset_mean=True, include_test=True)\n",
    "    if pca:\n",
    "        data_obj.transform_data_to_k_components(k_components, reset_V_k=True, include_test=True)\n",
    "        V_k_train = data_obj.V_k_train\n",
    "\n",
    "    # get the X_train, X_val, X_test\n",
    "    X_train, X_val, X_test = data_obj.X_train, data_obj.X_val, data_obj.X_test\n",
    "    y_m_test = data_obj.y_m_test\n",
    "    y_c_test = data_obj.y_c_test\n",
    "    y_c_2_test = data_obj.y_c_2_test\n",
    "\n",
    "    # get the V_c_1, V_c_2\n",
    "    V_c_1 = list_results_JSE_first_concept[seed]['V_c']\n",
    "    V_c_2 = list_results_JSE_second_concept[seed]['V_c']\n",
    "    P_1 = torch.eye(V_c_1.shape[0]) - create_P(V_c_1)\n",
    "    P_2 = torch.eye(V_c_2.shape[0]) - create_P(V_c_2)\n",
    "    d = V_c_1.shape[0]\n",
    "\n",
    "    # transform the data to remove first concept, then the second concept\n",
    "    X_train_transformed = torch.matmul(torch.matmul(X_train, P_1), P_2)\n",
    "    X_val_transformed = torch.matmul(torch.matmul(X_val, P_1), P_2)\n",
    "    X_test_transformed = torch.matmul(torch.matmul(X_test, P_1), P_2)\n",
    "    data_obj.reset_X(X_train_transformed, X_val_transformed, batch_size=batch_size, reset_X_objects=True, include_weights=False, train_weights = None, val_weights = None, only_main=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Train the model to get main\n",
    "    set_seed(seed)\n",
    "    main_model_JSE_both = return_linear_model(d, \n",
    "                                               data_obj.main_loader,\n",
    "                                              device,\n",
    "                                              solver = solver,\n",
    "                                              lr=lr_ERM,\n",
    "                                              per_step=per_step,\n",
    "                                              tol = optimizer_settings['tol'],\n",
    "                                              early_stopping = early_stopping,\n",
    "                                              patience = optimizer_settings['patience'],\n",
    "                                              epochs = epochs,\n",
    "                                              bias=True,\n",
    "                                              weight_decay=weight_decay_ERM, \n",
    "                                              model_name=dataset+'_main_model')\n",
    "    \n",
    "    # get the accuracy of the main model\n",
    "    y_m_pred_test_ERM = main_model_JSE_both(X_test)\n",
    "\n",
    "    # get the accuracy of the main model overall \n",
    "    main_acc_after = get_acc_pytorch_model(y_m_test, y_m_pred_test_ERM)\n",
    "\n",
    "    # get the accuracy of the main model per group - first concept\n",
    "    result_per_group_1, _ = get_acc_per_group(y_m_pred_test_ERM, y_m_test, y_c_test)\n",
    "\n",
    "    # get the accuracy of the main model per group - second concept\n",
    "    result_per_group_2, _ = get_acc_per_group(y_m_pred_test_ERM, y_m_test, y_c_2_test)\n",
    "\n",
    "    result_dict_JSE_both = {\n",
    "        'main_acc_after': main_acc_after,\n",
    "        'result_per_group_1': result_per_group_1,\n",
    "        'result_per_group_2': result_per_group_2}\n",
    "    \n",
    "    list_results_JSE_both.append(result_dict_JSE_both)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'celebA'\n",
    "dataset_setting = 'sampled_data_two_concepts'\n",
    "device = 'cpu'\n",
    "spurious_ratio=0.8\n",
    "single_finetuned_model = False\n",
    "demean=True\n",
    "pca=True \n",
    "k_components=300\n",
    "balanced_training_second_concept = False\n",
    "batch_size = 128\n",
    "settings = 'standard_ERM_settings_GW'\n",
    "lr_ERM = optimizer_info[settings][dataset]['lr']\n",
    "weight_decay_ERM = optimizer_info[settings][dataset]['weight_decay']\n",
    "epochs = 50\n",
    "expected_diff = 0.0\n",
    "early_stopping = True\n",
    "solver='SGD'\n",
    "per_step=50\n",
    "\n",
    "\n",
    " # set the settings for dataset\n",
    "dataset_settings = data_info[dataset][dataset_setting]\n",
    "optimizer_settings = optimizer_info['All']\n",
    "\n",
    "\n",
    "# get the data obj\n",
    "list_results_GW_ERM = []\n",
    "for seed in range(0, 5):\n",
    "\n",
    "    # get the data obj\n",
    "    set_seed(seed)\n",
    "    data_obj = get_dataset_obj(dataset, dataset_settings, spurious_ratio, data_info, seed, device,  single_model_for_embeddings=single_finetuned_model)\n",
    "\n",
    "    # demean, pca\n",
    "    if demean:\n",
    "        data_obj.demean_X(reset_mean=True, include_test=True)\n",
    "    if pca:\n",
    "        data_obj.transform_data_to_k_components(k_components, reset_V_k=True, include_test=True)\n",
    "        V_k_train = data_obj.V_k_train\n",
    "\n",
    "    # get the X_train, X_val, X_test\n",
    "    X_train, X_val, X_test = data_obj.X_train, data_obj.X_val, data_obj.X_test\n",
    "\n",
    "    # get the y_m_test\n",
    "    y_m_test = data_obj.y_m_test\n",
    "    y_c_test = data_obj.y_c_test\n",
    "    y_c_2_test = data_obj.y_c_2_test\n",
    "\n",
    "    # use group-balance weights for the second concept\n",
    "    include_weights = True\n",
    "    main_weights_train, main_weights_val = data_obj.get_group_weights(normalized=True, second_concept=True)\n",
    "    \n",
    "    # reset the data objects\n",
    "    data_obj.reset_X(X_train, X_val, batch_size=batch_size, reset_X_objects=False, include_weights=include_weights, train_weights = main_weights_train, val_weights = main_weights_val)\n",
    "    d = X_train.shape[1]\n",
    "\n",
    "    # Train the model to get main\n",
    "    set_seed(seed)\n",
    "    main_model_GW_ERM = return_linear_model(d, \n",
    "                                               data_obj.main_loader,\n",
    "                                              device,\n",
    "                                              solver = solver,\n",
    "                                              lr=lr_ERM,\n",
    "                                              per_step=per_step,\n",
    "                                              tol = optimizer_settings['tol'],\n",
    "                                              early_stopping = early_stopping,\n",
    "                                              patience = optimizer_settings['patience'],\n",
    "                                              epochs = epochs,\n",
    "                                              bias=True,\n",
    "                                              weight_decay=weight_decay_ERM, \n",
    "                                              model_name=dataset+'_main_model')\n",
    "    \n",
    "    # get the accuracy of the main model\n",
    "    y_m_pred_test_GW = main_model_GW_ERM(X_test)\n",
    "\n",
    "    # get the accuracy of the main model overall \n",
    "    main_acc_after = get_acc_pytorch_model(y_m_test, y_m_pred_test_GW)\n",
    "\n",
    "    # get the accuracy of the main model per group - first concept\n",
    "    result_per_group_1, _ = get_acc_per_group(y_m_pred_test_GW, y_m_test, y_c_test)\n",
    "\n",
    "    # get the accuracy of the main model per group - second concept\n",
    "    result_per_group_2, _ = get_acc_per_group(y_m_pred_test_GW, y_m_test, y_c_2_test)\n",
    "\n",
    "    result_dict_GW_ERM = {\n",
    "        'main_acc_after': main_acc_after,\n",
    "        'result_per_group_1': result_per_group_1,\n",
    "        'result_per_group_2': result_per_group_2}\n",
    "    \n",
    "    list_results_GW_ERM.append(result_dict_GW_ERM)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'celebA'\n",
    "dataset_setting = 'sampled_data_two_concepts'\n",
    "device = 'cpu'\n",
    "spurious_ratio=0.8\n",
    "single_finetuned_model = False\n",
    "demean=True\n",
    "pca=True \n",
    "k_components=300\n",
    "balanced_training_second_concept = False\n",
    "batch_size = 128\n",
    "settings = 'standard_ERM_settings'\n",
    "lr_ERM = optimizer_info[settings][dataset]['lr']\n",
    "weight_decay_ERM = optimizer_info[settings][dataset]['weight_decay']\n",
    "epochs = 50\n",
    "alpha = 0.05\n",
    "expected_diff = 0.0\n",
    "early_stopping = True\n",
    "solver='SGD'\n",
    "per_step=50\n",
    "\n",
    "\n",
    " # set the settings for dataset\n",
    "dataset_settings = data_info[dataset][dataset_setting]\n",
    "optimizer_settings = optimizer_info['All']\n",
    "\n",
    "\n",
    "# get the data obj\n",
    "\n",
    "list_results_ERM = []\n",
    "for seed in range(0, 5):\n",
    "\n",
    "    # get the data obj\n",
    "    set_seed(seed)\n",
    "    data_obj = get_dataset_obj(dataset, dataset_settings, spurious_ratio, data_info, seed, device,  single_model_for_embeddings=single_finetuned_model)\n",
    "\n",
    "    # demean, pca\n",
    "    if demean:\n",
    "        data_obj.demean_X(reset_mean=True, include_test=True)\n",
    "    if pca:\n",
    "        data_obj.transform_data_to_k_components(k_components, reset_V_k=True, include_test=True)\n",
    "        V_k_train = data_obj.V_k_train\n",
    "\n",
    "    # get the X_train, X_val, X_test\n",
    "    X_train, X_val, X_test = data_obj.X_train, data_obj.X_val, data_obj.X_test\n",
    "\n",
    "    # get the y_m_test\n",
    "    y_m_test = data_obj.y_m_test\n",
    "    y_c_test = data_obj.y_c_test\n",
    "    y_c_2_test = data_obj.y_c_2_test\n",
    "\n",
    "    # use group-balance weights for the second concept\n",
    "    include_weights = False\n",
    "    main_weights_train, main_weights_val = None, None\n",
    "\n",
    "    # reset the data objects\n",
    "    data_obj.reset_X(X_train, X_val, batch_size=batch_size, reset_X_objects=False, include_weights=include_weights, train_weights = main_weights_train, val_weights = main_weights_val)\n",
    "    d = X_train.shape[1]\n",
    "\n",
    "    # Train the model to get main\n",
    "    set_seed(seed)\n",
    "    main_model_ERM = return_linear_model(d, \n",
    "                                               data_obj.main_loader,\n",
    "                                              device,\n",
    "                                              solver = solver,\n",
    "                                              lr=lr_ERM,\n",
    "                                              per_step=per_step,\n",
    "                                              tol = optimizer_settings['tol'],\n",
    "                                              early_stopping = early_stopping,\n",
    "                                              patience = optimizer_settings['patience'],\n",
    "                                              epochs = epochs,\n",
    "                                              bias=True,\n",
    "                                              weight_decay=weight_decay_ERM, \n",
    "                                              model_name=dataset+'_main_model')\n",
    "    \n",
    "    # get the accuracy of the main model\n",
    "    y_m_pred_test_ERM = main_model_ERM(X_test)\n",
    "\n",
    "    # get the accuracy of the main model overall \n",
    "    main_acc_after = get_acc_pytorch_model(y_m_test, y_m_pred_test_ERM)\n",
    "\n",
    "    # get the accuracy of the main model per group - first concept\n",
    "    result_per_group_1, _ = get_acc_per_group(y_m_pred_test_ERM, y_m_test, y_c_test)\n",
    "\n",
    "    # get the accuracy of the main model per group - second concept\n",
    "    result_per_group_2, _ = get_acc_per_group(y_m_pred_test_ERM, y_m_test, y_c_2_test)\n",
    "\n",
    "    result_dict_ERM = {\n",
    "        'main_acc_after': main_acc_after,\n",
    "        'result_per_group_1': result_per_group_1,\n",
    "        'result_per_group_2': result_per_group_2}\n",
    "    \n",
    "    list_results_ERM.append(result_dict_ERM)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_entry_method(list_results, multiplier=100):\n",
    "\n",
    "    # loop over the results\n",
    "    acc = []\n",
    "    worst_acc_1 =[]\n",
    "    worst_acc_2 = []\n",
    "    result_per_group_1 = []\n",
    "    result_per_group_2 = []\n",
    "    for result_dict in list_results:\n",
    "        acc.append(result_dict['main_acc_after'])\n",
    "        \n",
    "        # from the results per group_1, get the worst accuracy\n",
    "        wg_1 = result_dict['result_per_group_1']['mean'].values.min()\n",
    "        wg_2 = result_dict['result_per_group_2']['mean'].values.min()\n",
    "        worst_acc_1.append(wg_1)\n",
    "        worst_acc_2.append(wg_2)\n",
    "        \n",
    "        # get the accuracy per group\n",
    "        result_per_group_1.append(result_dict['result_per_group_1']['mean'].values)\n",
    "        result_per_group_2.append(result_dict['result_per_group_2']['mean'].values)\n",
    "    \n",
    "    # get the average acc, and the se\n",
    "    avg_acc = np.mean(acc)\n",
    "    se_acc = np.std(acc)/np.sqrt(len(result_per_group_1))\n",
    "\n",
    "    # get the average wg acc, and the se\n",
    "    avg_worst_acc_1 = np.mean(worst_acc_1)\n",
    "    se_worst_acc_1 = np.std(worst_acc_1)/np.sqrt(len(result_per_group_1))\n",
    "    avg_worst_acc_2 = np.mean(worst_acc_2)\n",
    "    se_worst_acc_2 = np.std(worst_acc_2)/np.sqrt(len(result_per_group_2))\n",
    "\n",
    "\n",
    "    # get the average acc per group\n",
    "    result_per_group_1 = np.array(result_per_group_1)\n",
    "    result_per_group_2 = np.array(result_per_group_2)\n",
    "    avg_acc_per_group_1 = np.mean(result_per_group_1, axis=0)\n",
    "    avg_acc_per_group_2 = np.mean(result_per_group_2, axis=0)\n",
    "    se_acc_per_group_1 = np.std(result_per_group_1, axis=0)/np.sqrt(len(result_per_group_1))\n",
    "    se_acc_per_group_2 = np.std(result_per_group_2, axis=0)/np.sqrt(len(result_per_group_2))\n",
    "\n",
    "    # create table entry - take the average accuracy and put the se in brackets\n",
    "    acc = str(round(avg_acc*multiplier, 2)) + ' (' + str(round(se_acc*multiplier, 2)) + ')'\n",
    "    worst_acc_1 = str(round(avg_worst_acc_1*multiplier, 2)) + ' (' + str(round(se_worst_acc_1*multiplier, 2)) + ')'\n",
    "    worst_acc_2 = str(round(avg_worst_acc_2*multiplier, 2)) + ' (' + str(round(se_worst_acc_2*multiplier, 2)) + ')' \n",
    "    acc_per_group_1 = [str(round(avg_acc_per_group_1[i]*multiplier, 2)) + ' (' + str(round(se_acc_per_group_1[i]*multiplier, 2)) + ')' for i in range(len(avg_acc_per_group_1))]\n",
    "    acc_per_group_2 = [str(round(avg_acc_per_group_2[i]*multiplier, 2)) + ' (' + str(round(se_acc_per_group_2[i]*multiplier, 2)) + ')' for i in range(len(avg_acc_per_group_2))]\n",
    "\n",
    "    return acc, worst_acc_1, worst_acc_2, acc_per_group_1, acc_per_group_2,\n",
    "\n",
    "\n",
    "def create_table(list_results_JSE_both, list_results_JSE_both_order_1, list_results_JSE_both_order_2, list_results_JSE_first_concept, list_results_JSE_second_concept, list_ERM_results, list_ERM_GW_results):\n",
    "\n",
    "    # get the number of samples in test set\n",
    "    n_samples_1 = list_results_JSE_both_order_1[0]['result_per_group_1']['count']\n",
    "    n_samples_2 = list_results_JSE_both_order_1[0]['result_per_group_2']['count']\n",
    "\n",
    "    # JSE with both concepts\n",
    "    acc_JSE_both, worst_acc_1_JSE_both, worst_acc_2_JSE_both, acc_per_group_1_JSE_both, acc_per_group_2_JSE_both = create_table_entry_method(list_results_JSE_both)\n",
    "\n",
    "    # JSE with first concept\n",
    "    acc_JSE_first_concept, worst_acc_1_JSE_first_concept, worst_acc_2_JSE_first_concept, acc_per_group_1_JSE_first_concept, acc_per_group_2_JSE_first_concept = create_table_entry_method(list_results_JSE_first_concept)\n",
    "\n",
    "    # JSE with second concept\n",
    "    acc_JSE_second_concept, worst_acc_1_JSE_second_concept, worst_acc_2_JSE_second_concept, acc_per_group_1_JSE_second_concept, acc_per_group_2_JSE_second_concept = create_table_entry_method(list_results_JSE_second_concept)\n",
    "\n",
    "    # GW-ERM\n",
    "    acc_GW_ERM, worst_acc_1_GW_ERM, worst_acc_2_GW_ERM, acc_per_group_1_GW_ERM, acc_per_group_2_GW_ERM = create_table_entry_method(list_ERM_GW_results)\n",
    "\n",
    "    # ERM\n",
    "    acc_ERM, worst_acc_1_ERM, worst_acc_2_ERM, acc_per_group_1_ERM, acc_per_group_2_ERM = create_table_entry_method(list_ERM_results)\n",
    "\n",
    "    # first table - all results for concept 1\n",
    "    table = pd.DataFrame({\n",
    "        'Method': [r'JSE (both)' , r'JSE ( $\\ysp^{(1)}$)', r'JSE ( $\\ysp^{(2)}$)', 'ERM', 'GW-ERM'],\n",
    "        'Accuracy': [acc_JSE_both,  acc_JSE_first_concept, acc_JSE_second_concept, acc_ERM, acc_GW_ERM],\n",
    "        'Worst group 1': [worst_acc_1_JSE_both, worst_acc_1_JSE_first_concept, worst_acc_1_JSE_second_concept, worst_acc_1_ERM, worst_acc_1_GW_ERM],\n",
    "        'Worst group 2': [worst_acc_2_JSE_both, worst_acc_2_JSE_first_concept, worst_acc_2_JSE_second_concept, worst_acc_2_ERM, worst_acc_2_GW_ERM]})\n",
    "    \n",
    "    # second table - rows are groups, columns are methods, showing result per group\n",
    "    table_per_group_1 = pd.DataFrame({\n",
    "        'Group': ['Group 1', 'Group 2', 'Group 3', 'Group 4'],\n",
    "        'n': n_samples_1,\n",
    "        r'JSE (both)': acc_per_group_1_JSE_both,\n",
    "        r'JSE ( $\\ysp^{(1)}$)': acc_per_group_1_JSE_first_concept,\n",
    "        r'JSE ( $\\ysp^{(2)}$)': acc_per_group_1_JSE_second_concept,\n",
    "        'ERM': acc_per_group_1_ERM,\n",
    "        'GW-ERM': acc_per_group_1_GW_ERM})\n",
    "    \n",
    "    # third table - rows are groups, columns are methods, showing result per group\n",
    "    table_per_group_2 = pd.DataFrame({\n",
    "        'Group': ['Group 1', 'Group 2', 'Group 3', 'Group 4'],\n",
    "        'n': n_samples_2,\n",
    "        r'JSE (both)': acc_per_group_2_JSE_both,\n",
    "        r'JSE ( $\\ysp^{(1)}$)': acc_per_group_2_JSE_first_concept,\n",
    "        r'JSE ( $\\ysp^{(2)}$)': acc_per_group_2_JSE_second_concept,\n",
    "        'ERM': acc_per_group_2_ERM,\n",
    "        'GW-ERM': acc_per_group_2_GW_ERM})\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    return table, table_per_group_1, table_per_group_2\n",
    "\n",
    "\n",
    "table, table_per_group_1, table_per_group_2 = create_table(list_results_JSE_both, list_results_JSE_first_concept, list_results_JSE_second_concept, list_results_ERM, list_results_GW_ERM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# order main-concept to be (1-1), (1-0), (0-1), (0-0)\n",
    "table_per_group_1_ordered = table_per_group_1.copy()\n",
    "table_per_group_2_ordered = table_per_group_2.copy()\n",
    "table_per_group_1_ordered = table_per_group_1_ordered.iloc[[3, 2, 1, 0], :]\n",
    "table_per_group_2_ordered = table_per_group_2_ordered.iloc[[3, 2, 1, 0], :]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JSE-replicate-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
